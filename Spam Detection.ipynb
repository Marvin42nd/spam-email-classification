{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b533dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from python_utils import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478a8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./datasets/\"\n",
    "\n",
    "hidden_size = 500\n",
    "embed = 200\n",
    "tar_size = 2\n",
    "batch_size = 3\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(db_name, batch_size):\n",
    "    \"\"\"\n",
    "    Load the csv datasets into torchtext files\n",
    "\n",
    "    Inputs:\n",
    "    db_name (string)\n",
    "       The name of the dataset. This name must correspond to the folder name.\n",
    "    batch_size\n",
    "       The batch size\n",
    "    \"\"\"\n",
    "    print(\"Loading \" + db_name + \"...\")\n",
    "\n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "    LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "    tv_datafields = [(\"sentence\", TEXT),\n",
    "                     (\"label\", LABEL)]\n",
    "\n",
    "    trn, vld = TabularDataset.splits(\n",
    "        path=DATA_ROOT + db_name,  # the root directory where the data lies\n",
    "        train='train.csv', validation=\"test.csv\",\n",
    "        format='csv',\n",
    "        skip_header=False,\n",
    "        fields=tv_datafields)\n",
    "\n",
    "    TEXT.build_vocab(trn)\n",
    "\n",
    "    print(\"vocab size: %i\" % len(TEXT.vocab))\n",
    "\n",
    "    train_iter, val_iter = BucketIterator.splits(\n",
    "        (trn, vld),\n",
    "        batch_sizes=(batch_size, batch_size),\n",
    "        device=-1,  # specify dont use gpu\n",
    "        sort_key=lambda x: len(x.sentence),  # sort the sentences by length\n",
    "        sort_within_batch=False,\n",
    "        repeat=False)\n",
    "\n",
    "    return train_iter, val_iter, len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e0bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(db):\n",
    "    data = pd.read_csv(db)\n",
    "    data = data.dropna()\n",
    "    data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, embed, hidden_size, tar_size, len_vocab, batch_size):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(len_vocab, embed)\n",
    "        self.lstmm = nn.LSTM(embed, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, tar_size)\n",
    "        self.hidd = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda(),\n",
    "                Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda())\n",
    "\n",
    "    def forward(self, sentence, batch_size):\n",
    "\n",
    "        # print(\"Sentence shape\", sentence.shape)\n",
    "\n",
    "        # print(self.embedding(sentence).view(len(sentence), 1, -1).shape)\n",
    "        # print(self.hidd)\n",
    "        # y = self.embedding(sentence).view(len(sentence),1,-1).shape\n",
    "        # h0 = Variable(torch.zeros([1, batch_size, hidden_size]), requires_grad=False)\n",
    "        # c0 = Variable(torch.zeros([1, batch_size, hidden_size]), requires_grad=False)\n",
    "        # if use_gpu:\n",
    "        #    h0 = h0.cuda()\n",
    "        # print(\"Hidden\", h0.shape)\n",
    "        # if use_gpu:\n",
    "        #   c0 = c0.cuda()\n",
    "\n",
    "        output, (hn, cn) = self.lstmm(self.embedding(sentence), self.hidd)  #\n",
    "        self.hidd = (hn, cn)\n",
    "        # print(\"output\", output.shape)\n",
    "        # print(\"Hidden\", hn.shape)\n",
    "        # print(Variable(self.embedding(sentence)).shape)\n",
    "        lin = self.fc(output[-1, :, :])\n",
    "        # print(lin.shape)\n",
    "        return lin\n",
    "\n",
    "    def fit(self, train_iterator):\n",
    "        # switch to train mode\n",
    "        self.train()\n",
    "\n",
    "        # define loss function\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "        # setup SGD\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "\n",
    "        for epoch in range(25):  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, batch in enumerate(train_iterator, 0):\n",
    "                sentence = batch.sentence\n",
    "                label = batch.label\n",
    "\n",
    "                if use_gpu:\n",
    "                    sentence = sentence.cuda()\n",
    "                    label = label.cuda()\n",
    "\n",
    "                # wrap them in Variable\n",
    "                # sentence, label = Variable(sentence), Variable(label)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                self.zero_grad()\n",
    "\n",
    "                self.hidd = self.init_hidden()\n",
    "\n",
    "                # compute forward pass\n",
    "                outputs = self.forward(sentence, batch_size=self.batch_size)\n",
    "\n",
    "                # get loss function\n",
    "                loss = criterion(outputs, label)\n",
    "\n",
    "                # do backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # do one gradient step\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.data[0]\n",
    "\n",
    "            print('[Epoch: %d] loss: %.3f' %\n",
    "                  (epoch + 1, running_loss / (i + 1)))\n",
    "            # print(\"Loss\", running_loss)\n",
    "            running_loss = 0.0\n",
    "            #self.predict(test_iterator)\n",
    "\n",
    "        print('Finished Training')\n",
    "\n",
    "        #return pred_labels, test_labels\n",
    "\n",
    "    def predict(self, test_iterator):\n",
    "        # switch to evaluate mode\n",
    "        self.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_predicted = []\n",
    "        labels = []\n",
    "        for batch in test_iterator:\n",
    "            sentence = batch.sentence\n",
    "            label = batch.label\n",
    "            if use_gpu:\n",
    "                sentence = sentence.cuda()\n",
    "                #label = label.cuda()\n",
    "            outputs = self.forward(sentence, batch_size=batch_size)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "            #print(\"Pred\", Variable(predicted).data.cpu().numpy().shape)\n",
    "            #print(\"labels\", label.data.cpu().numpy().shape)\n",
    "\n",
    "            correct += (Variable(predicted).data.cpu().numpy() == label.data.cpu().numpy()).sum()\n",
    "            all_predicted += predicted.tolist()\n",
    "            labels += label.data.cpu().numpy().tolist()\n",
    "        total = 1293\n",
    "        print('Accuracy of the network on the 1293 test images: %d %%' % (\n",
    "            100 * correct / total))\n",
    "        #print(\"Corr\", correct)\n",
    "        #print(\"Pred\", all_predicted)\n",
    "        #print(\"label\", label.data.cpu().numpy)\n",
    "\n",
    "        return all_predicted, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # get data\n",
    "\n",
    "    train_iterator, test_iterator, len1 = load_dataset(\"spam\", batch_size)\n",
    "    print(train_iterator)\n",
    "    print(torch.cuda.device_count())\n",
    "    len_vocab = len1\n",
    "    # full net\n",
    "    print(\"LSTM Network\")\n",
    "    model = BaseNet(embed, hidden_size, tar_size, len_vocab, batch_size)\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "    model.fit(train_iterator)\n",
    "    pred_labels, test_labels = model.predict(test_iterator)\n",
    "    #plt.figure(1)\n",
    "    #print(\"pred\",pred_labels)\n",
    "    #print(\"test\",test_labels)\n",
    "    plot_confusion_matrix(pred_labels, test_labels, \"LSTM_Spam\")\n",
    "    plt.show()\n",
    "    plt.savefig('ConfusionMatrix_LSTM_Spam.png', )\t\n",
    "    torch.save(model, 'LSTM_Spam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4994596",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451687e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
